{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return(roc_auc_score(y_test, y_pred, average=average))\n",
    "\n",
    "\n",
    "\n",
    "class classif(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, Train, Test, Target):\n",
    "        \n",
    "   \n",
    "        self.Train = Train.copy()\n",
    "        self.Test = Test\n",
    "        self.Target = Target.copy()\n",
    "        self.dim_test = Test.shape\n",
    "        self.dim_train = Train.shape\n",
    "        \n",
    "        self.AUC = make_scorer(multiclass_roc_auc_score)\n",
    "    \n",
    "\n",
    "        \n",
    "    def best_clf(self, params = False, ID = 'ID', cv = 3, cv_grid = 3, n = 10000, n_grid = 10000, value = 0, \n",
    "                 view_nan = False, grid = False):\n",
    "        \n",
    "        self.Train.drop([ID], axis = 1, inplace = True)   \n",
    "        self.Test.drop([ID], axis = 1, inplace = True)\n",
    "        self.Target.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "        if view_nan:\n",
    "            \n",
    "            print(\"Missing Values :\\n\")\n",
    "        \n",
    "            total = self.Train.isnull().sum().sort_values(ascending=False)\n",
    "            percent = (self.Train.isnull().sum()/self.Train.isnull().count()).sort_values(ascending=False)*100\n",
    "            missing_data = pd.concat([total,percent], axis=1, keys=['Total', '%'])\n",
    "            print('Train')\n",
    "            print(\"{} \\n\".format(missing_data[(percent>0)]))\n",
    "\n",
    "\n",
    "            total = self.Test.isnull().sum().sort_values(ascending=False)\n",
    "            percent = (self.Test.isnull().sum()/self.Test.isnull().count()).sort_values(ascending=False)*100\n",
    "            missing_data = pd.concat([total,percent], axis=1, keys=['Total', '%'])\n",
    "            print('Test')\n",
    "            print(\"{} \\n\".format(missing_data[(percent>0)]))\n",
    "        \n",
    "        \n",
    "        if type(value) == int:\n",
    "            self.Train.fillna(value, inplace = True)\n",
    "            self.Test.fillna(value, inplace = True)\n",
    "            #self.Missing_values()\n",
    "        \n",
    "        elif value == 'bfill':\n",
    "            self.Train.fillna('bfill', inplace = True)\n",
    "            self.Test.fillna('bfill', inplace = True)\n",
    "            #self.Missing_values()                        \n",
    "        \n",
    "        elif value == 'ffill':\n",
    "            self.Train.fillna('ffill', inplace = True)\n",
    "            self.Test.fillna('ffill', inplace = True)\n",
    "            #self.Missing_values()\n",
    "        \n",
    "        \n",
    "        if self.Train.isnull().any().any() == False & self.Test.isnull().any().any() == False :\n",
    "            print('\\n Train & Test NaN data filled by {} \\n'.format(value))\n",
    "        else :\n",
    "            print('Fail to fill Train et Test')\n",
    "                \n",
    "        \n",
    "        for i in self.Train.columns:###########\n",
    "            \n",
    "            if self.Train[i].dtype == float:\n",
    "                self.Train[i] = self.Train[i].astype('int')\n",
    "            \n",
    "            if self.Train[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(self.Train[i]) + list(self.Test[i]))\n",
    "                self.Train[i] = encoder.transform(list(self.Train[i]))\n",
    "                self.Test[i] = encoder.transform(list(self.Test[i]))\n",
    "                \n",
    "        for i in self.Test.columns:###########\n",
    "            if self.Test[i].dtype == float:\n",
    "                self.Test[i] = self.Test[i].astype('int')\n",
    "        \n",
    "        for i in self.Target.columns:\n",
    "            if self.Target[i].dtype == object:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(list(self.Target[i]))\n",
    "                self.Target[i] = le.transform(list(self.Target[i]))\n",
    "                \n",
    "\n",
    "                \n",
    "        X_tr,X_te,Y_tr,Y_te = train_test_split(self.Train,self.Target, random_state = 0, test_size = 1/3)\n",
    "        \n",
    "        \n",
    "        print('\\n Searching best classifier... \\n')\n",
    "        \n",
    "        clfs = {}\n",
    "        clfs['Bagging'] = {'clf': BaggingClassifier(), 'name' : 'Bagging'}\n",
    "        clfs['Gradient Boosting'] = {'clf': GradientBoostingClassifier(), 'name' : 'Gradient Boosting'}\n",
    "        clfs['XGBoost'] = {'clf': XGBClassifier(), 'name' : 'XGBoost'}\n",
    "        clfs['Random Forest'] = {'clf': RandomForestClassifier(n_estimators = 100, n_jobs=-1), 'name' : 'Random Forest'}\n",
    "        clfs['Decision Tree'] = {'clf': DecisionTreeClassifier(), 'name' : 'Decision Tree'}\n",
    "        clfs['KNN'] = {'clf': KNeighborsClassifier(n_jobs=-1), 'name': 'KNN'}\n",
    "        #clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "        #clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "        clfs['SVM'] = {'clf': SVC(gamma = 'auto'), 'name' : 'SVM'}\n",
    "        \n",
    "        for item in clfs:\n",
    "            \n",
    "            Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.ravel(Y_tr[0:n]), \n",
    "                                                 cv=cv, scoring = self.AUC)\n",
    "            Score_mean = Score.mean()\n",
    "            STD2 = Score.std()*2\n",
    "\n",
    "            clfs[item]['score'] = Score # roc_auc\n",
    "            clfs[item]['mean'] = Score_mean\n",
    "            clfs[item]['std2'] = STD2\n",
    "            \n",
    "            \n",
    "            print(\"{} \\n\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(), \n",
    "                                                                              clfs[item]['score'].std()*2)))\n",
    "        \n",
    "        \n",
    "        Best_clf = clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['name']\n",
    "        \n",
    "       # print(clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['name'])\n",
    "        \n",
    "        \n",
    "        if grid :\n",
    "            #print('grid = True')\n",
    "            \n",
    "            if params == False:\n",
    "                #print('params = False')\n",
    "                \n",
    "                #print(Best_clf)\n",
    "                \n",
    "                if Best_clf == 'Gradient Boosting' :\n",
    "                 #   print('Best_clf = gb')\n",
    "                    \n",
    "                    params = {'n_estimators' : [100, 300, 600], \n",
    "                              'max_depth' : [5, 10, None],\n",
    "                             'learning_rate' : [.001, .01, .1]} \n",
    "                    \n",
    "                    \n",
    "                elif Best_clf == 'Random Forest':\n",
    "                  #  print('Best_clf = dt ou rf')\n",
    "                    \n",
    "                    params = {'n_estimators' : [10,100, 300], \n",
    "                              'max_depth' : [5, 10, None],\n",
    "                             'criterion' : ['gini', 'entropy']}\n",
    "                    \n",
    "                elif Best_clf == 'Decision Tree' : \n",
    "                   # print('best_clf = dt')\n",
    "                    \n",
    "                    params = {'max_depth' : [5, 10, 50, None],\n",
    "                             'criterion' : ['gini', 'entropy']}\n",
    "                    \n",
    "                elif Best_clf == 'XGBoost':\n",
    "                    #print('Best_clf = xgb')\n",
    "                    \n",
    "                    params = {'eta' : [.01,.1,.3], \n",
    "                              'max_depth' : [5, 10, None],\n",
    "                             'gamma' : [0, .1, .01]}       \n",
    "                    \n",
    "                elif Best_clf == 'Bagging':\n",
    "                    #print('best_clf = bag)')\n",
    "                       \n",
    "                    \n",
    "                    params = {'n_estimators' : [100, 300, 600]} \n",
    "                \n",
    "                elif Best_clf == 'KNN':\n",
    "                    \n",
    "                    params = {'n_neighbors' : [2,5, 10, 30, 40],\n",
    "                             'p' : [1,2]}\n",
    "                    \n",
    "                elif Best_clf == 'SVM' :\n",
    "                    \n",
    "                    params = {'C' : {1, .5, .1, 5},\n",
    "                             'tol' : [.01, .001, .1, .0001]}\n",
    "                    \n",
    "            \n",
    "                \n",
    "            print('\\n Searching best hyperparametres of {} Classifier among : \\n'.format(Best_clf))\n",
    "            print('{} \\n'.format(params))\n",
    "            #print('Starting GridSearchCV using {} Classifier with {} folds \\n'.format(Best_clf, cv_grid))\n",
    "            \n",
    "            \n",
    "            clf = clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['clf']\n",
    "            gr = GridSearchCV(clf, param_grid = params, cv = cv_grid, scoring = self.AUC, n_jobs=-1, \n",
    "                              verbose = 1, refit = True, iid = True) #;\n",
    "\n",
    "            gr.fit(X_tr[0:n_grid], np.ravel(Y_tr[0:n_grid]))\n",
    "\n",
    "            \n",
    "            print(' Best score :', gr.best_score_,   '\\n Using this parametres :', gr.best_params_)\n",
    "            return(gr)\n",
    "            \n",
    "\n",
    "    \n",
    "    def grid(self, clf, params, cv = 3, n = 100000):\n",
    "        \n",
    "        \n",
    "        X_tr,X_te,Y_tr,Y_te = train_test_split(self.Train,self.Target, random_state = 0, test_size = 1/3)\n",
    "        \n",
    "        gr = GridSearchCV(clf, param_grid = params, cv = cv, scoring = self.AUC, n_jobs=-1, \n",
    "                          verbose = 1, refit = True, iid = True);\n",
    "        \n",
    "        gr.fit(X_tr[0:n], np.ravel(Y_tr[0:n]))\n",
    "        \n",
    "        #print(' Best score :', gr.best_score_,   '\\n Using this parametres :', gr.best_params_, '\\n With :', clf)\n",
    "        print(' Best score :', gr.best_score_,   '\\n Using this parametres :', gr.best_params_, \n",
    "              '\\n With : \\n {} '.format(clf))\n",
    "        return gr\n",
    "        \n",
    "    def feature_eng(self, Test, value = 0, ID = 'ID'):\n",
    "        \n",
    "        Test.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "        if type(value) == int:\n",
    "            Test.fillna(value, inplace = True)\n",
    "        \n",
    "        elif value == 'bfill':\n",
    "                Test.fillna('bfill', inplace = True)\n",
    "        \n",
    "        elif value == 'ffill':\n",
    "            Test.fillna('ffill', inplace = True)\n",
    "            \n",
    "        \n",
    "        for i in Test.columns:###########\n",
    "            if Test[i].dtype == float:\n",
    "                Test[i] =Test[i].astype('int')\n",
    "                \n",
    "            elif Test[i].dtype == object:\n",
    "                    encoder = LabelEncoder()\n",
    "                    encoder.fit(list(Test[i]))\n",
    "                    Test[i] = encoder.transform(list(Test[i]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pred(self, Test, gr, prob = False, same = True, ID = 'ID', value = 0): #\n",
    "        \n",
    "        #Test.drop([ID], axis = 1, inplace = True)\n",
    "        Pred = pd.DataFrame() \n",
    "\n",
    "        if same == False :\n",
    "            \n",
    "            Test.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "            if type(value) == int:\n",
    "                Test.fillna(value, inplace = True)\n",
    "\n",
    "            elif value == 'bfill':\n",
    "                    Test.fillna('bfill', inplace = True)\n",
    "\n",
    "            elif value == 'ffill':\n",
    "                Test.fillna('ffill', inplace = True)\n",
    "\n",
    "\n",
    "            for i in Test.columns:\n",
    "                if Test[i].dtype == float:\n",
    "                    Test[i] =Test[i].astype('int')\n",
    "\n",
    "                elif Test[i].dtype == object:\n",
    "                        encoder = LabelEncoder()\n",
    "                        encoder.fit(list(Test[i]))\n",
    "                        Test[i] = encoder.transform(list(Test[i]))   \n",
    "        \n",
    "        if prob == False :\n",
    "            #Pred[ID] = Test[ID]\n",
    "            Pred['Target'] = gr.predict(Test)\n",
    "            return(Pred)\n",
    "\n",
    "        else :\n",
    "            return(gr.predict_proba(Test))\n",
    "\n",
    "       # else :\n",
    "        #    return(gr.predict_proba(self.feature_eng(Data, value , ID)))\n",
    "       \n",
    "      \n",
    "    \n",
    "    \n",
    "\t\t\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Rakuten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "Train\n",
      "                      Total       %\n",
      "WARRANTIES_PRICE      96603  96.603\n",
      "SHIPPING_PRICE        67610  67.610\n",
      "BUYER_BIRTHDAY_DATE    5836   5.836\n",
      "SHIPPING_MODE           315   0.315\n",
      "PRICECLUB_STATUS         57   0.057\n",
      "SELLER_SCORE_AVERAGE      6   0.006\n",
      "SELLER_SCORE_COUNT        6   0.006 \n",
      "\n",
      "Test\n",
      "                      Total          %\n",
      "WARRANTIES_PRICE      96688  96.692835\n",
      "SHIPPING_PRICE        67430  67.433372\n",
      "BUYER_BIRTHDAY_DATE    5785   5.785289\n",
      "SHIPPING_MODE           357   0.357018\n",
      "PRICECLUB_STATUS         73   0.073004\n",
      "SELLER_SCORE_AVERAGE     13   0.013001\n",
      "SELLER_SCORE_COUNT       13   0.013001 \n",
      "\n",
      "\n",
      " Train & Test NaN data filled by 0 \n",
      "\n",
      "\n",
      " Searching best classifier... \n",
      "\n",
      "Bagging: 0.5353 (+/- 0.0081) \n",
      "\n",
      "Gradient Boosting: 0.5424 (+/- 0.0086) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.5363 (+/- 0.0175) \n",
      "\n",
      "Random Forest: 0.5393 (+/- 0.0187) \n",
      "\n",
      "Decision Tree: 0.5491 (+/- 0.0236) \n",
      "\n",
      "KNN: 0.5182 (+/- 0.0040) \n",
      "\n",
      "SVM: 0.5029 (+/- 0.0040) \n",
      "\n",
      "\n",
      " Searching best hyperparametres of Decision Tree Classifier among : \n",
      "\n",
      "{'max_depth': [5, 10, 50, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "#%run -i 'classif.py'\n",
    "\n",
    "\n",
    "Train = pd.read_csv('Train.csv', sep = ',')\n",
    "Test = pd.read_csv('Test.csv', sep = ',')\n",
    "Test1 = pd.read_csv('Test.csv', sep = ',')\n",
    "Target = pd.read_csv('Target.csv', sep =';')\n",
    "\n",
    "CLF = classif(Train, Test, Target)\n",
    "\n",
    "\n",
    "params = {'n_estimators' : [50,100, 300], 'max_depth' : [3,5, None]} #,0.05,0.1]}\n",
    "\n",
    "gr = CLF.best_clf(params = False , n = 1000, n_grid = 1000, view_nan = True, grid = True)\n",
    "#gr = CLF.best_clf(params = False , n = 1000, n_grid = 100, view_nan = True, grid = True, pred_Test = True, prob = True)\n",
    "\n",
    "\n",
    "#CLF.pred(Test,gr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gr = CLF.grid(clf, cv = 2, n = 1000, params = params)\n",
    "\n",
    "#CLF.pred(Test, gr, same = True, ID = 'ID', value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CLF.pred(Test, gr, same = True, ID = 'ID', value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sogec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pd.read_csv('Train_sg.csv', sep = ';')\n",
    "Test = pd.read_csv('Test_sg.csv', sep = ';')\n",
    "Target = pd.read_csv('Target_sg.csv', sep =';')\n",
    "\n",
    "CLF = classif(Train, Test, Target)\n",
    "\n",
    "\n",
    "params = {'n_estimators' : [50,100, 300], 'max_depth' : [3,5, None]} #,0.05,0.1]}\n",
    "\n",
    "gr = CLF.best_clf(params = False , n = 1000, n_grid = 50, view_nan = True, grid = True)\n",
    "\n",
    "#CLF.pred(Test,gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return(roc_auc_score(y_test, y_pred, average=average))\n",
    "\n",
    "\n",
    "\n",
    "class BestEstimator(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, Data, Target):\n",
    "        \n",
    "   \n",
    "        self.Data = Data.copy()\n",
    "        self.Target = Target.copy()\n",
    "        self.dim_ = Data.shape\n",
    "        \n",
    "        self.AUC = make_scorer(multiclass_roc_auc_score)\n",
    "    \n",
    "\n",
    "        \n",
    "    def best_clf(self,\n",
    "                 type_esti = 'classifier', # Type of estimator : classifier or regressor\n",
    "                 params = False,           # Allow to use a custom hyperparametres dict for GridSearCV\n",
    "                 ID = 'ID',                # ID feature of the DataFrame used\n",
    "                 target_ID = True,         # If Target feature have an ID\n",
    "                 cv = 3,                   # Numbers of folds for the first estimators check\n",
    "                 grid = False,             # if True, use a GridSearchCV with best estimator found\n",
    "                 cv_grid = 3,              # Number of folds for the GridSearchCV \n",
    "                 n = 10000,                # Number of observations used for the first check\n",
    "                 n_grid = 10000,           # Number of observations used for the GridSearchCV\n",
    "                 value = 0,                # Value for fill Nan\n",
    "                 view_nan = False,         # if True check the NaN Data\n",
    "                 scorer = 'mae'):          # Type of scorer is type_esti = 'regressor'      \n",
    "                 \n",
    "        \n",
    "        self.Data.drop([ID], axis = 1, inplace = True)\n",
    "        if target_ID :\n",
    "            self.Target.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "        if view_nan:\n",
    "            \n",
    "            print(\"Missing Values :\\n\")\n",
    "        \n",
    "            total = self.Data.isnull().sum().sort_values(ascending=False)\n",
    "            percent = (self.Data.isnull().sum()/self.Data.isnull().count()).sort_values(ascending=False)*100\n",
    "            missing_data = pd.concat([total,percent], axis=1, keys=['Total', '%'])\n",
    "            print(\"{} \\n\".format(missing_data[(percent>0)]))\n",
    "        \n",
    "        \n",
    "        if type(value) == int:\n",
    "            self.Data.fillna(value, inplace = True)\n",
    "            #self.Test.fillna(value, inplace = True)\n",
    "            #self.Missing_values()\n",
    "        \n",
    "        elif value == 'bfill':\n",
    "            self.Data.fillna('bfill', inplace = True)\n",
    "            #self.Test.fillna('bfill', inplace = True)\n",
    "            #self.Missing_values()                        \n",
    "        \n",
    "        elif value == 'ffill':\n",
    "            self.Data.fillna('ffill', inplace = True)\n",
    "            #self.Test.fillna('ffill', inplace = True)\n",
    "            #self.Missing_values()\n",
    "        \n",
    "        \n",
    "        if self.Data.isnull().any().any() == False :\n",
    "            print('\\n NaN data filled by {} \\n'.format(value))\n",
    "        else :\n",
    "            print('Fail to fill NaN data')\n",
    "                \n",
    "        \n",
    "        for i in self.Data.columns:###########\n",
    "            \n",
    "            if self.Data[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(self.Data[i]))\n",
    "                self.Data[i] = encoder.transform(list(self.Data[i]))\n",
    "            \n",
    "            if self.Data[i].dtype == float:\n",
    "                self.Data[i] = self.Data[i].astype('int')\n",
    "                \n",
    "        \n",
    "        for i in self.Target.columns:\n",
    "            if self.Target[i].dtype == object:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(list(self.Target[i]))\n",
    "                self.Target[i] = le.transform(list(self.Target[i]))\n",
    "                \n",
    "\n",
    "                \n",
    "        X_tr,X_te,Y_tr,Y_te = train_test_split(self.Data,self.Target, random_state = 0, test_size = 1/3)\n",
    "        \n",
    "        \n",
    "        if type_esti == 'classifier':\n",
    "        \n",
    "            print('\\n Searching for the best classifier on {} data... \\n'.format(n))\n",
    "\n",
    "            clfs = {}\n",
    "            clfs['Bagging'] = {'clf': BaggingClassifier(), 'name' : 'Bagging'}\n",
    "            clfs['Gradient Boosting'] = {'clf': GradientBoostingClassifier(), 'name' : 'Gradient Boosting'}\n",
    "            clfs['XGBoost'] = {'clf': XGBClassifier(), 'name' : 'XGBoost'}\n",
    "            clfs['Random Forest'] = {'clf': RandomForestClassifier(n_estimators = 100, n_jobs=-1), 'name' : 'Random Forest'}\n",
    "            clfs['Decision Tree'] = {'clf': DecisionTreeClassifier(), 'name' : 'Decision Tree'}\n",
    "            clfs['KNN'] = {'clf': KNeighborsClassifier(n_jobs=-1), 'name': 'KNN'}\n",
    "            #clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "            #clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "            clfs['SVM'] = {'clf': SVC(gamma = 'auto'), 'name' : 'SVM'}\n",
    "\n",
    "            for item in clfs:\n",
    "\n",
    "                Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.ravel(Y_tr[0:n]), \n",
    "                                                     cv=cv, scoring = self.AUC)\n",
    "                Score_mean = Score.mean()\n",
    "                STD2 = Score.std()*2\n",
    "\n",
    "                clfs[item]['score'] = Score # roc_auc\n",
    "                clfs[item]['mean'] = Score_mean\n",
    "                clfs[item]['std2'] = STD2\n",
    "\n",
    "\n",
    "                print(\"{} \\n\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(), \n",
    "                                                                                  clfs[item]['score'].std()*2)))\n",
    "\n",
    "\n",
    "            Best_clf = clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['name']\n",
    "\n",
    "           # print(clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['name'])\n",
    "\n",
    "\n",
    "            if grid :\n",
    "                #print('grid = True')\n",
    "\n",
    "                if params == False:\n",
    "                    #print('params = False')\n",
    "\n",
    "                    #print(Best_clf)\n",
    "\n",
    "                    if Best_clf == 'Gradient Boosting' :\n",
    "                     #   print('Best_clf = gb')\n",
    "\n",
    "                        params = {'n_estimators' : [100, 300, 600], \n",
    "                                  'max_depth' : [5, 10, None],\n",
    "                                 'learning_rate' : [.001, .01, .1]} \n",
    "\n",
    "\n",
    "                    elif Best_clf == 'Random Forest':\n",
    "                      #  print('Best_clf = dt ou rf')\n",
    "\n",
    "                        params = {'n_estimators' : [10,100, 300], \n",
    "                                  'max_depth' : [5, 10, None],\n",
    "                                 'criterion' : ['gini', 'entropy']}\n",
    "\n",
    "                    elif Best_clf == 'Decision Tree' : \n",
    "                       # print('best_clf = dt')\n",
    "\n",
    "                        params = {'max_depth' : [5, 10, 50, None],\n",
    "                                 'criterion' : ['gini', 'entropy']}\n",
    "\n",
    "                    elif Best_clf == 'XGBoost':\n",
    "                        #print('Best_clf = xgb')\n",
    "\n",
    "                        params = {'eta' : [.01,.1,.3], \n",
    "                                  'max_depth' : [5, 10, None],\n",
    "                                 'gamma' : [0, .1, .01]}       \n",
    "\n",
    "                    elif Best_clf == 'Bagging':\n",
    "                        #print('best_clf = bag)')\n",
    "\n",
    "\n",
    "                        params = {'n_estimators' : [100, 300, 600]} \n",
    "\n",
    "                    elif Best_clf == 'KNN':\n",
    "\n",
    "                        params = {'n_neighbors' : [2,5, 10, 30, 40],\n",
    "                                 'p' : [1,2]}\n",
    "\n",
    "                    elif Best_clf == 'SVM' :\n",
    "\n",
    "                        params = {'C' : {1, .5, .1, 5},\n",
    "                                 'tol' : [.01, .001, .1, .0001]}\n",
    "\n",
    "\n",
    "\n",
    "                print('\\n Searching best hyperparametres of {} Classifier on {} data among : \\n'.format(Best_clf, n_grid))\n",
    "                print('{} \\n'.format(params))\n",
    "                #print('Starting GridSearchCV using {} Classifier with {} folds \\n'.format(Best_clf, cv_grid))\n",
    "\n",
    "\n",
    "                clf = clfs[max(clfs.keys(), key = (lambda k: clfs[k]['mean']))]['clf']\n",
    "                gr = GridSearchCV(clf, param_grid = params, cv = cv_grid, scoring = self.AUC, n_jobs=-1, \n",
    "                                  verbose = 1, refit = True, iid = True) #;\n",
    "\n",
    "                gr.fit(X_tr[0:n_grid], np.ravel(Y_tr[0:n_grid]))\n",
    "\n",
    "\n",
    "                #print(' Best score :', gr.best_score_,   '\\n Using these parametres :', gr.best_params_)\n",
    "                \n",
    "            #####\n",
    "                print('\\n Finally, best estimator is : {} Classifier'.format(Best_clf), '\\n Using these parametres :', gr.best_params_)\n",
    "            #####\n",
    "                return(gr)\n",
    "        elif type_esti == 'regressor':\n",
    "            pass\n",
    "            \n",
    "            # A développer !!!!\n",
    "\n",
    "    \n",
    "    def grid(self, clf, params, cv = 3, n = 100000):\n",
    "        \n",
    "        \n",
    "        X_tr,X_te,Y_tr,Y_te = train_test_split(self.Data,self.Target, random_state = 0, test_size = 1/3)\n",
    "        \n",
    "        gr = GridSearchCV(clf, param_grid = params, cv = cv, scoring = self.AUC, n_jobs=-1, \n",
    "                          verbose = 1, refit = True, iid = True);\n",
    "        \n",
    "        gr.fit(X_tr[0:n], np.ravel(Y_tr[0:n]))\n",
    "        \n",
    "        #print(' Best score :', gr.best_score_,   '\\n Using this parametres :', gr.best_params_, '\\n With :', clf)\n",
    "        print(' Best score on Train:', gr.best_score_,   '\\n Using this parametres :', gr.best_params_, \n",
    "              '\\n With : \\n {} '.format(clf))\n",
    "        return gr\n",
    "        \n",
    "    def feature_eng(self, Test, value = 0, ID = 'ID'):\n",
    "        \n",
    "        Test.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "        if type(value) == int:\n",
    "            Test.fillna(value, inplace = True)\n",
    "        \n",
    "        elif value == 'bfill':\n",
    "                Test.fillna('bfill', inplace = True)\n",
    "        \n",
    "        elif value == 'ffill':\n",
    "            Test.fillna('ffill', inplace = True)\n",
    "            \n",
    "        \n",
    "        for i in Test.columns:###########\n",
    "            if Test[i].dtype == float:\n",
    "                Test[i] =Test[i].astype('int')\n",
    "                \n",
    "            elif Test[i].dtype == object:\n",
    "                    encoder = LabelEncoder()\n",
    "                    encoder.fit(list(Test[i]))\n",
    "                    Test[i] = encoder.transform(list(Test[i]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pred(self, Test, gr, prob = False, same = True, ID = 'ID', value = 0): #\n",
    "        \n",
    "        #Test.drop([ID], axis = 1, inplace = True)\n",
    "        Pred = pd.DataFrame() \n",
    "\n",
    "        if same == False :\n",
    "            \n",
    "            Test.drop([ID], axis = 1, inplace = True)\n",
    "        \n",
    "            if type(value) == int:\n",
    "                Test.fillna(value, inplace = True)\n",
    "\n",
    "            elif value == 'bfill':\n",
    "                    Test.fillna('bfill', inplace = True)\n",
    "\n",
    "            elif value == 'ffill':\n",
    "                Test.fillna('ffill', inplace = True)\n",
    "\n",
    "\n",
    "            for i in Test.columns:\n",
    "                if Test[i].dtype == float:\n",
    "                    Test[i] =Test[i].astype('int')\n",
    "\n",
    "                elif Test[i].dtype == object:\n",
    "                        encoder = LabelEncoder()\n",
    "                        encoder.fit(list(Test[i]))\n",
    "                        Test[i] = encoder.transform(list(Test[i]))   \n",
    "        \n",
    "        if prob == False :\n",
    "            #Pred[ID] = Test[ID]\n",
    "            Pred['Target'] = gr.predict(Test)\n",
    "            return(Pred)\n",
    "\n",
    "        else :\n",
    "            return(gr.predict_proba(Test))\n",
    "\n",
    "       # else :\n",
    "        #    return(gr.predict_proba(self.feature_eng(Data, value , ID)))\n",
    "       \n",
    "      \n",
    "    \n",
    "    \n",
    "\t\t\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "                      Total       %\n",
      "WARRANTIES_PRICE      96603  96.603\n",
      "SHIPPING_PRICE        67610  67.610\n",
      "BUYER_BIRTHDAY_DATE    5836   5.836\n",
      "SHIPPING_MODE           315   0.315\n",
      "PRICECLUB_STATUS         57   0.057\n",
      "SELLER_SCORE_AVERAGE      6   0.006\n",
      "SELLER_SCORE_COUNT        6   0.006 \n",
      "\n",
      "\n",
      " NaN data filled by 0 \n",
      "\n",
      "\n",
      " Searching for the best classifier on 20000 data... \n",
      "\n",
      "Bagging: 0.5501 (+/- 0.0120) \n",
      "\n",
      "Gradient Boosting: 0.5428 (+/- 0.0091) \n",
      "\n",
      "Random Forest: 0.5542 (+/- 0.0096) \n",
      "\n",
      "Decision Tree: 0.5512 (+/- 0.0029) \n",
      "\n",
      "KNN: 0.5215 (+/- 0.0033) \n",
      "\n",
      "SVM: 0.5119 (+/- 0.0002) \n",
      "\n",
      "\n",
      " Searching for best hyperparametres of Random Forest Classifier on 20000 data among : \n",
      "\n",
      "{'n_estimators': [10, 100, 300], 'max_depth': [5, 10, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, best estimator is : Random Forest Classifier \n",
      " Using these parametres : {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}\n",
      "--- 346.3958389759064 seconds ---\n"
     ]
    }
   ],
   "source": [
    "%run -i bestestimator.py\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "Train = pd.read_csv('Train.csv', sep = ',')\n",
    "Target = pd.read_csv('Target.csv', sep =';')\n",
    "\n",
    "CLF = BestEstimator(Train, Target)\n",
    "\n",
    "\n",
    "params = {'n_estimators' : [50,100, 300], 'max_depth' : [3,5, None]} #,0.05,0.1]}\n",
    "\n",
    "\n",
    "gr = CLF.best_clf(type_esti = 'classifier',ID = 'ID', \n",
    "                  params = False , n = 20000, n_grid = 20000, \n",
    "                  view_nan = True, grid = True, target_ID = True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "            Total      %\n",
      "note          114  0.570\n",
      "diplome       110  0.550\n",
      "dispo         106  0.530\n",
      "cheveux       103  0.515\n",
      "sexe          100  0.500\n",
      "exp            96  0.480\n",
      "salaire        95  0.475\n",
      "specialite     93  0.465\n",
      "age            91  0.455\n",
      "date           91  0.455 \n",
      "\n",
      "\n",
      " NaN data filled by 0 \n",
      "\n",
      "\n",
      " Searching for the best classifier on 200000 data... \n",
      "\n",
      "Bagging: 0.6529 (+/- 0.0051) \n",
      "\n",
      "Gradient Boosting: 0.5661 (+/- 0.0237) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.5484 (+/- 0.0078) \n",
      "\n",
      "Random Forest: 0.6444 (+/- 0.0077) \n",
      "\n",
      "Decision Tree: 0.6739 (+/- 0.0180) \n",
      "\n",
      "KNN: 0.5023 (+/- 0.0026) \n",
      "\n",
      "SVM: 0.5000 (+/- 0.0000) \n",
      "\n",
      "\n",
      " Searching for best hyperparametres of Decision Tree Classifier on 200000 data among : \n",
      "\n",
      "{'max_depth': [5, 10, 50, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      " Finally, best estimator is : Decision Tree Classifier \n",
      " Using these parametres : {'criterion': 'gini', 'max_depth': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    7.1s finished\n"
     ]
    }
   ],
   "source": [
    "%run -i bestestimator.py\n",
    "\n",
    "Data = pd.read_csv('data.csv', sep = ',', index_col = 0)\n",
    "Target_data = pd.DataFrame()\n",
    "Target_data['Target'] = Data['embauche']\n",
    "Data.drop(['embauche'], axis = 1, inplace = True)\n",
    "\n",
    "CLF = BestEstimator(Data, Target_data)\n",
    "\n",
    "gr = CLF.best_clf(type_esti = 'classifier',ID = 'index', \n",
    "                  params = False , n = 200000, n_grid = 200000, \n",
    "                  view_nan = True, grid = True, target_ID = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "                            Total         %\n",
      "1_diffClosing stocks(kmt)     261  2.569151\n",
      "2_diffClosing stocks(kmt)     257  2.529777\n",
      "3_diffClosing stocks(kmt)     253  2.490403\n",
      "4_diffClosing stocks(kmt)     249  2.451029\n",
      "5_diffClosing stocks(kmt)     245  2.411655\n",
      "6_diffClosing stocks(kmt)     241  2.372281\n",
      "7_diffClosing stocks(kmt)     238  2.342750\n",
      "8_diffClosing stocks(kmt)     235  2.313220\n",
      "9_diffClosing stocks(kmt)     232  2.283689\n",
      "10_diffClosing stocks(kmt)    228  2.244315\n",
      "11_diffClosing stocks(kmt)    224  2.204941\n",
      "12_diffClosing stocks(kmt)    220  2.165567\n",
      "1_diffImports(kmt)            151  1.486367\n",
      "2_diffImports(kmt)            148  1.456836\n",
      "3_diffImports(kmt)            145  1.427306\n",
      "4_diffImports(kmt)            142  1.397775\n",
      "5_diffImports(kmt)            139  1.368245\n",
      "6_diffImports(kmt)            136  1.338714\n",
      "7_diffImports(kmt)            133  1.309184\n",
      "8_diffImports(kmt)            130  1.279654\n",
      "9_diffImports(kmt)            127  1.250123\n",
      "10_diffImports(kmt)           124  1.220593\n",
      "11_diffImports(kmt)           121  1.191062\n",
      "12_diffImports(kmt)           118  1.161532 \n",
      "\n",
      "\n",
      " NaN data filled by 0 \n",
      "\n",
      "\n",
      " Searching for the best classifier on 20000 data... \n",
      "\n",
      "Bagging: 0.6580 (+/- 0.0107) \n",
      "\n",
      "Gradient Boosting: 0.6848 (+/- 0.0179) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.6846 (+/- 0.0151) \n",
      "\n",
      "Random Forest: 0.6868 (+/- 0.0195) \n",
      "\n",
      "Decision Tree: 0.6199 (+/- 0.0214) \n",
      "\n",
      "KNN: 0.5738 (+/- 0.0096) \n",
      "\n",
      "SVM: 0.4998 (+/- 0.0006) \n",
      "\n",
      "\n",
      " Searching for best hyperparametres of Random Forest Classifier on 20000 data among : \n",
      "\n",
      "{'n_estimators': [10, 100, 300], 'max_depth': [5, 10, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   50.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, best estimator is : Random Forest Classifier \n",
      " Using these parametres : {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "%run -i bestestimator.py\n",
    "\n",
    "Train_v2 = pd.read_csv('Train1.csv', sep = ';')\n",
    "Target_v2 =  pd.read_csv('Target1.csv', sep = ';')\n",
    "\n",
    "CLF = BestEstimator(Train_v2, Target_v2)\n",
    "\n",
    "gr = CLF.best_clf(type_esti = 'classifier',ID = 'ID', \n",
    "                  params = False , n = 20000, n_grid = 20000, \n",
    "                  view_nan = True, grid = True, target_ID = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
