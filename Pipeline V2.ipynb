{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%run -i bestestimator_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, \\\n",
    "    RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return (roc_auc_score(y_test, y_pred, average=average))\n",
    "\n",
    "\n",
    "class BestEstimator(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 type_esti = 'classifier', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3):\n",
    "        \n",
    "        self.type_esti = type_esti\n",
    "        #self.params = params\n",
    "        self.cv = cv\n",
    "        self.grid = grid\n",
    "        self.hard_grid = hard_grid\n",
    "        self.cv_grid = cv_grid\n",
    "        \n",
    "        self.AUC = make_scorer(multiclass_roc_auc_score)\n",
    "        \n",
    "        self.Decision_Function = None\n",
    "        self.gr = None\n",
    "        self.estim = None\n",
    "        self.Target = None\n",
    "        self.Data = None\n",
    "        \n",
    "\n",
    "    def fit(self, data, target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 1000,\n",
    "            n_grid = 1000,\n",
    "            value_nan = 0,\n",
    "            view_nan = True,\n",
    "           params = False,\n",
    "           value = 0,\n",
    "           scoring = 'AUC'):\n",
    "        \n",
    "        loss = scoring \n",
    "        self.Data = data.copy()\n",
    "        self.Target = target.copy()\n",
    "\n",
    "        self.Data.drop([ID], axis=1, inplace=True)\n",
    "        if target_ID:\n",
    "            self.Target.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "        if view_nan:\n",
    "            print(\"Missing Values :\\n\")\n",
    "\n",
    "            total = self.Data.isnull().sum().sort_values(ascending=False)\n",
    "            percent = (self.Data.isnull().sum() / self.Data.isnull().count()).sort_values(ascending=False) * 100\n",
    "            missing_data = pd.concat([total, percent], axis=1, keys=['Total', '%'])\n",
    "            print(\"{} \\n\".format(missing_data[(percent > 0)]))\n",
    "\n",
    "        if type(value) == int:\n",
    "            self.Data.fillna(value, inplace=True)\n",
    "            # self.Test.fillna(value, inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        elif value == 'bfill':\n",
    "            self.Data.fillna('bfill', inplace=True)\n",
    "            # self.Test.fillna('bfill', inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        elif value == 'ffill':\n",
    "            self.Data.fillna('ffill', inplace=True)\n",
    "            # self.Test.fillna('ffill', inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        if self.Data.isnull().any().any() == False:\n",
    "            print('NaN data filled by {} \\n'.format(value))\n",
    "        else:\n",
    "            print('Fail to fill NaN data')\n",
    "\n",
    "        for i in self.Data.columns:  ###########\n",
    "\n",
    "            if self.Data[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(self.Data[i]))\n",
    "                self.Data[i] = encoder.transform(list(self.Data[i]))\n",
    "\n",
    "            if self.Data[i].dtype == float:\n",
    "                self.Data[i] = self.Data[i].astype('int')\n",
    "\n",
    "        for i in self.Target.columns:\n",
    "            if self.Target[i].dtype == object:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(list(self.Target[i]))\n",
    "                self.Target[i] = le.transform(list(self.Target[i]))\n",
    "\n",
    "        X_tr, X_te, Y_tr, Y_te = train_test_split(self.Data, self.Target, random_state=0, test_size=1 / 3)\n",
    "\n",
    "        print('Searching for the best regressor on {} data using {} loss... \\n'.format(n, scoring))\n",
    "\n",
    "        if self.type_esti == 'classifier':\n",
    "\n",
    "            # print('\\n Searching for the best classifier on {} data... \\n'.format(n))\n",
    "\n",
    "            clfs = {}\n",
    "            clfs['Bagging'] = {'clf': BaggingClassifier(), 'name': 'Bagging'}\n",
    "            clfs['Gradient Boosting'] = {'clf': GradientBoostingClassifier(), 'name': 'Gradient Boosting'}\n",
    "            clfs['XGBoost'] = {'clf': XGBClassifier(), 'name': 'XGBoost'}\n",
    "            clfs['Random Forest'] = {'clf': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                                     'name': 'Random Forest'}\n",
    "            clfs['Decision Tree'] = {'clf': DecisionTreeClassifier(), 'name': 'Decision Tree'}\n",
    "            clfs['Extra Tree'] = {'clf': ExtraTreesClassifier(n_jobs=-1), 'name': 'Extra Tree'}\n",
    "\n",
    "            clfs['KNN'] = {'clf': KNeighborsClassifier(n_jobs=-1), 'name': 'KNN'}\n",
    "            # clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "            # clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "            clfs['SVM'] = {'clf': SVC(gamma='auto'), 'name': 'SVM'}\n",
    "\n",
    "            \n",
    "            if scoring == 'AUC' and np.unique(self.Target).shape[0] > 2:\n",
    "                scoring = self.AUC\n",
    "                score = 'AUC'\n",
    "            else :\n",
    "                score = 'AUC'\n",
    "                scoring = 'roc_auc'\n",
    "            \n",
    "            for item in clfs:\n",
    "                \n",
    "                Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.ravel(Y_tr[0:n]),\n",
    "                                        cv=self.cv, scoring=scoring)\n",
    "               \n",
    "                Score_mean = Score.mean()\n",
    "                STD2 = Score.std() * 2\n",
    "\n",
    "                clfs[item]['score'] = Score  # roc_auc\n",
    "                clfs[item]['mean'] = Score_mean\n",
    "                clfs[item]['std2'] = STD2\n",
    "\n",
    "                print(\"\\n {}\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(),\n",
    "                                                                     clfs[item]['score'].std() * 2)))\n",
    "\n",
    "            Best_clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['name']\n",
    "\n",
    "\n",
    "        elif self.type_esti == 'regressor':\n",
    "\n",
    "            clfs = {}\n",
    "            clfs['Bagging'] = {'clf': BaggingRegressor(), 'name': 'Bagging'}\n",
    "            clfs['Gradient Boosting'] = {'clf': GradientBoostingRegressor(), 'name': 'Gradient Boosting'}\n",
    "            clfs['XGBoost'] = {'clf': XGBRegressor(), 'name': 'XGBoost'}\n",
    "            clfs['Random Forest'] = {'clf': RandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "                                     'name': 'Random Forest'}\n",
    "            clfs['Decision Tree'] = {'clf': DecisionTreeRegressor(), 'name': 'Decision Tree'}\n",
    "            clfs['Extra Tree'] = {'clf': ExtraTreesRegressor(n_jobs=-1), 'name': 'Extra Tree'}\n",
    "            clfs['KNN'] = {'clf': KNeighborsRegressor(n_jobs=-1), 'name': 'KNN'}\n",
    "            # clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "            # clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "            clfs['SVM'] = {'clf': SVR(gamma='auto'), 'name': 'SVM'}\n",
    "\n",
    "            for item in clfs:\n",
    "                # print(Y_tr[0:30])\n",
    "                Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.array(np.ravel(Y_tr[0:n])),\n",
    "                                        ########\"\"\n",
    "                                        cv=self.cv, scoring=scoring)\n",
    "                Score_mean = Score.mean()\n",
    "                STD2 = Score.std() * 2\n",
    "\n",
    "                clfs[item]['score'] = Score  # roc_auc\n",
    "                clfs[item]['mean'] = Score_mean\n",
    "                clfs[item]['std2'] = STD2\n",
    "\n",
    "                print(\"\\n {}\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(),\n",
    "                                                                     clfs[item]['score'].std() * 2)))\n",
    "\n",
    "            Best_clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['name']\n",
    "\n",
    "        if self.grid:\n",
    "            # print('grid = True')\n",
    "\n",
    "            if params == False:\n",
    "                # print('params = False')\n",
    "\n",
    "                # print(Best_clf)\n",
    "\n",
    "                if self.hard_grid == False:\n",
    "\n",
    "                    if Best_clf == 'Extra Tree':\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600],\n",
    "                                      'criterion': ['mse', 'mae'],\n",
    "                                      'max_depth': [None, 5, 10]}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600],\n",
    "                                      'criterion': ['gini', 'entropy'],\n",
    "                                      'max_depth': [None, 5, 10]}\n",
    "\n",
    "                    if Best_clf == 'Gradient Boosting':\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600],\n",
    "                                      'max_depth': [5, 10, None],\n",
    "                                      'learning_rate': [.001, .01, .1],\n",
    "                                      'loss': ['ls', 'lad']}\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600],\n",
    "                                      'max_depth': [5, 10, None],\n",
    "                                      'learning_rate': [.001, .01, .1],\n",
    "                                      'loss': ['deviance', 'exponential']}\n",
    "\n",
    "\n",
    "                    elif Best_clf == 'Random Forest':\n",
    "                        #  print('Best_clf = dt ou rf')\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300],\n",
    "                                      'max_depth': [5, 10, None],\n",
    "                                      'criterion': ['mse', 'mae']}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300],\n",
    "                                      'max_depth': [5, 10, None],\n",
    "                                      'criterion': ['gini', 'entropy']}\n",
    "\n",
    "                    elif Best_clf == 'Decision Tree':\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'max_depth': [5, 10, 50, None],\n",
    "                                      'criterion': ['mse', 'friedman_mse', 'mae']}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'max_depth': [5, 10, 50, None],\n",
    "                                      'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "                    elif Best_clf == 'XGBoost':\n",
    "                        # print('Best_clf = xgb')\n",
    "\n",
    "                        params = {'eta': [.01, .1, .3],\n",
    "                                  'max_depth': [5, 10, 15],\n",
    "                                  'gamma': [0, .1, .01]}\n",
    "\n",
    "                    elif Best_clf == 'Bagging':\n",
    "                        # print('best_clf = bag)')\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600]}\n",
    "\n",
    "                    elif Best_clf == 'KNN':\n",
    "\n",
    "                        params = {'n_neighbors': [2, 5, 10, 30, 40],\n",
    "                                  'p': [1, 2]}\n",
    "\n",
    "                    elif Best_clf == 'SVM':\n",
    "\n",
    "                        params = {'C': {1, .5, .1, 5},\n",
    "                                  'tol': [.01, .001, .1, .0001]}\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if Best_clf == 'Extra Tree':\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                      'criterion': ['mae', 'mse'],\n",
    "                                      'max_depth': [None, 5, 10, 15, 20, 25]}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                      'criterion': ['gini', 'entropy'],\n",
    "                                      'max_depth': [None, 5, 10, 15, 20, 25]}\n",
    "\n",
    "                    if Best_clf == 'Gradient Boosting':\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600, 1000, 1200],\n",
    "                                      'max_depth': [5, 10, 15, 25, None],\n",
    "                                      'learning_rate': [.001, .01, .1],\n",
    "                                      'loss': ['ls', 'lad', 'huber', 'quantile'],\n",
    "                                      'criterion': ['mse', 'friedman_mse']}\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [100, 300, 600, 1000, 1200],\n",
    "                                      'max_depth': [5, 10, 15, 25, None],\n",
    "                                      'learning_rate': [.001, .01, .1],\n",
    "                                      'loss': ['deviance', 'exponential'],\n",
    "                                      'criterion': ['mse', 'friedman_mse']}\n",
    "\n",
    "\n",
    "                    elif Best_clf == 'Random Forest':\n",
    "                        #  print('Best_clf = dt ou rf')\n",
    "\n",
    "                        if self.type_esti == 'regressor':\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                      'max_depth': [5, 10, 15, 20, 25, None],\n",
    "                                      'criterion': ['mse', 'mae']}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                      'max_depth': [5, 10, 15, 20, 25],\n",
    "                                      'criterion': ['gini', 'entropy']}\n",
    "\n",
    "                    elif Best_clf == 'Decision Tree':\n",
    "\n",
    "                        if params == 'regressor':\n",
    "\n",
    "                            params = {'max_depth': [5, 10, 50, 100, None],\n",
    "                                      'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "                                      'splitter': ['best', 'random']}\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            params = {'max_depth': [5, 10, 50, 100, None],\n",
    "                                      'criterion': ['gini', 'entropy'],\n",
    "                                      'splitter': ['best', 'random']}\n",
    "\n",
    "\n",
    "                    elif Best_clf == 'XGBoost':\n",
    "                        # print('Best_clf = xgb')\n",
    "\n",
    "                        params = {'eta': [0.001, .01, .1, .3, 1],\n",
    "                                  'max_depth': [5, 10, 15, 20, 25],\n",
    "                                  'gamma': [0, .1, .01, .001]}\n",
    "\n",
    "                    elif Best_clf == 'Bagging':\n",
    "                        # print('best_clf = bag)')\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600, 1000, 1200, 1500]}\n",
    "\n",
    "                    elif Best_clf == 'KNN':\n",
    "\n",
    "                        params = {'n_neighbors': [2, 5, 10, 30, 40, 70, 100],\n",
    "                                  'p': [1, 2, 3]}\n",
    "\n",
    "                    elif Best_clf == 'SVM':\n",
    "\n",
    "                        params = {'C': {1, .5, .1, 5, .01, .001},\n",
    "                                  'tol': [.01, .001, .1, .0001, 1],\n",
    "                                  'kernel': ['rbf', 'linear', 'poly', 'sigmoid', 'precomputed']}\n",
    "\n",
    "            if self.hard_grid:\n",
    "                print('\\n Searching for the best hyperparametres of {} using hard_grid on {} data among : \\n'.format(\n",
    "                    Best_clf, n_grid))\n",
    "\n",
    "            else:\n",
    "                print('\\n Searching for the best hyperparametres of {} on {} data among : \\n'.format(Best_clf, n_grid))\n",
    "            print('{} \\n'.format(params))\n",
    "            # print('Starting GridSearchCV using {} Classifier with {} folds \\n'.format(Best_clf, cv_grid))\n",
    "\n",
    "            \n",
    "        \n",
    "            clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['clf']\n",
    "\n",
    "            if loss == 'AUC' and np.unique(self.Target).shape[0] > 2:\n",
    "                \n",
    "                \n",
    "                self.gr = GridSearchCV(clf, param_grid=params, cv=self.cv_grid, scoring=scoring, \n",
    "                                verbose=1, refit=True, iid=True)\n",
    "            else :\n",
    "                self.gr = GridSearchCV(clf, param_grid=params, cv=self.cv_grid, scoring=scoring, \n",
    "                                    verbose=1, refit=True, iid=True, n_jobs = -1)\n",
    "\n",
    "            self.gr.fit(X_tr[0:n_grid], np.ravel(Y_tr[0:n_grid]))\n",
    "            \n",
    "\n",
    "\n",
    "            print('\\n Finally, the best estimator is : {} {}'.format(Best_clf, self.type_esti))\n",
    "\n",
    "            print('\\n Using these hyperparametres : {}'.format(self.gr.best_params_))\n",
    "\n",
    "            print('\\n With this {} score : {}'.format(loss, self.gr.best_score_))\n",
    "            \n",
    "            self.Decision_Function = self.gr.best_estimator_\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('\\n Best {} : {}'.format(self.type_esti, Best_clf))\n",
    "            \n",
    "            \n",
    "            \n",
    "    def ReFit(self, Train, Target, ID = 'ID', target_ID = 'ID', value = 0):\n",
    "        \n",
    "        train = Train.copy()\n",
    "        target = Target.copy()\n",
    "        \n",
    "        train = self.Transform(train, value = value, ID = ID)\n",
    "        target = self.Transform(target, value = value, ID = ID)\n",
    "        \n",
    "        self.estim = self.Decision_Function.fit(train, target)\n",
    "        \n",
    "        return(self.estim)\n",
    "\n",
    "    \n",
    "    \n",
    "    def Transform(self, Data, value=0, ID = 'ID'):\n",
    "        \n",
    "        Test = Data.copy()\n",
    "        \n",
    "        if ID != None:\n",
    "\n",
    "            Test.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "        if type(value) == int:\n",
    "            Test.fillna(value, inplace=True)\n",
    "\n",
    "        elif value == 'bfill':\n",
    "            Test.fillna('bfill', inplace=True)\n",
    "\n",
    "        elif value == 'ffill':\n",
    "            Test.fillna('ffill', inplace=True)\n",
    "\n",
    "        for i in Test.columns:  ###########\n",
    "            if Test[i].dtype == float:\n",
    "                Test[i] = Test[i].astype('int')\n",
    "\n",
    "            elif Test[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(Test[i]))\n",
    "                Test[i] = encoder.transform(list(Test[i]))\n",
    "        return(Test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pred_grid(self, Test, ID = 'ID', value = 0, prob = False):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pred(self, Test, ID=None, value = 0, target_ID = None, n = 100, prob = False):\n",
    "        \n",
    "        if ID == None:\n",
    "            test = self.Transform(Test, ID = None, value = 0).copy()\n",
    "        else :\n",
    "            test = self.Transform(Test, ID = ID, value = 0).copy()\n",
    "            \n",
    "    \n",
    "        if self.estim == None:\n",
    "            self.estim = self.ReFit(self.Data[0:n], self.Target[0:n], ID = None, target_ID = None, value = 0)\n",
    "            if prob == False:\n",
    "                pred = pd.DataFrame()\n",
    "                predict = self.estim.predict(test)\n",
    "                \n",
    "                if ID == None:\n",
    "                    pred['Target'] = predict\n",
    "                else :\n",
    "                    pred[ID] = Test[ID]\n",
    "                    pred['Target'] = predict\n",
    "                \n",
    "            else:\n",
    "                predict = self.estim.predict_proba(test)\n",
    "                \n",
    "                if ID == None:\n",
    "                    pred['Target'] = predict\n",
    "                else :\n",
    "                    pred[ID] = Test[ID]\n",
    "                    pred['Target'] = predict\n",
    "            \n",
    "        else:\n",
    "            if prob == False:\n",
    "                pred = pd.DataFrame()\n",
    "                predict = self.estim.predict(test)\n",
    "                \n",
    "                if ID == None:\n",
    "                    pred['Target'] = predict\n",
    "                else :\n",
    "                    pred[ID] = Test[ID]\n",
    "                    pred['Target'] = predict\n",
    "            else:\n",
    "                predict = self.estim.predict_proba(test)\n",
    "                \n",
    "                if ID == None:\n",
    "                    pred['Target'] = predict\n",
    "                else :\n",
    "                    pred[ID] = Test[ID]\n",
    "                    pred['Target'] = predict\n",
    "            \n",
    "        return(pred)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def grid(self, clf, params, cv=3, n=100000):\n",
    "\n",
    "        X_tr, X_te, Y_tr, Y_te = train_test_split(self.Data, self.Target, random_state=0, test_size=1 / 3)\n",
    "\n",
    "        gr = GridSearchCV(clf, param_grid=params, cv=cv, scoring=self.AUC, n_jobs=-1,\n",
    "                          verbose=1, refit=True, iid=True);\n",
    "\n",
    "        gr.fit(X_tr[0:n], np.ravel(Y_tr[0:n]))\n",
    "\n",
    "        # print(' Best score :', gr.best_score_,   '\\n Using this parametres :', gr.best_params_, '\\n With :', clf)\n",
    "        print(' Best score on Train:', gr.best_score_, '\\n Using this parametres :', gr.best_params_,\n",
    "              '\\n With : \\n {} '.format(clf))\n",
    "        return gr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Train.csv\", sep = ',')\n",
    "Target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Train1.csv\", sep = ';')\n",
    "Target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target1.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\input_training.csv\", sep = ';')\n",
    "Target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target_Engie.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN data filled by 0 \n",
      "\n",
      "Searching for the best regressor on 1000 data using AUC loss... \n",
      "\n",
      "\n",
      " Bagging: 0.6913 (+/- 0.0477)\n",
      "\n",
      " Gradient Boosting: 0.7192 (+/- 0.0291)\n",
      "\n",
      " XGBoost: 0.7121 (+/- 0.0225)\n",
      "\n",
      " Random Forest: 0.7185 (+/- 0.0319)\n",
      "\n",
      " Decision Tree: 0.6347 (+/- 0.0255)\n",
      "\n",
      " Extra Tree: 0.6768 (+/- 0.0226)\n",
      "\n",
      " KNN: 0.6367 (+/- 0.0594)\n",
      "\n",
      " SVM: 0.5040 (+/- 0.0054)\n",
      "\n",
      " Searching for the best hyperparametres of Gradient Boosting on 200 data among : \n",
      "\n",
      "{'n_estimators': [100, 300, 600], 'max_depth': [5, 10, None], 'learning_rate': [0.001, 0.01, 0.1], 'loss': ['deviance', 'exponential']} \n",
      "\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, the best estimator is : Gradient Boosting classifier\n",
      "\n",
      " Using these hyperparametres : {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 5, 'n_estimators': 600}\n",
      "\n",
      " With this AUC score : 0.6355889787664307\n"
     ]
    }
   ],
   "source": [
    "clf = BestEstimator(type_esti = 'classifier', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3)\n",
    "\n",
    "clf.fit(Train, Target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 1000,\n",
    "            n_grid = 200,\n",
    "            value_nan = 0,\n",
    "            view_nan = False,\n",
    "           params = False,\n",
    "        scoring = 'AUC') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = clf.Transform(Train, ID = 'ID', value = 0)  \n",
    "Target = clf.Transform(Target, ID = 'ID', value = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:425: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.ReFit(Train[0:50], Target[0:50], ID = 'ID', target_ID = 'ID', value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID00006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID00007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID00008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID00009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID00010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID00011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID00012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID00013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID00014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID00015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID00016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID00017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID00018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID00019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID00020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ID00021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ID00022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ID00023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ID00024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ID00025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ID00026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ID00027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ID00028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ID00029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ID00030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ID00031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ID00032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ID00033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ID00034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ID00035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ID00036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ID00037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ID00038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ID00039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ID00040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ID00041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ID00042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ID00043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ID00044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ID00045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ID00046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ID00047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ID00048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ID00049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ID00050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Target\n",
       "1   ID00002       0\n",
       "2   ID00003       0\n",
       "3   ID00004       0\n",
       "4   ID00005       1\n",
       "5   ID00006       1\n",
       "6   ID00007       0\n",
       "7   ID00008       0\n",
       "8   ID00009       0\n",
       "9   ID00010       0\n",
       "10  ID00011       0\n",
       "11  ID00012       0\n",
       "12  ID00013       0\n",
       "13  ID00014       0\n",
       "14  ID00015       1\n",
       "15  ID00016       0\n",
       "16  ID00017       0\n",
       "17  ID00018       0\n",
       "18  ID00019       0\n",
       "19  ID00020       0\n",
       "20  ID00021       0\n",
       "21  ID00022       0\n",
       "22  ID00023       0\n",
       "23  ID00024       0\n",
       "24  ID00025       1\n",
       "25  ID00026       0\n",
       "26  ID00027       1\n",
       "27  ID00028       1\n",
       "28  ID00029       0\n",
       "29  ID00030       0\n",
       "30  ID00031       1\n",
       "31  ID00032       0\n",
       "32  ID00033       1\n",
       "33  ID00034       1\n",
       "34  ID00035       1\n",
       "35  ID00036       0\n",
       "36  ID00037       1\n",
       "37  ID00038       0\n",
       "38  ID00039       0\n",
       "39  ID00040       1\n",
       "40  ID00041       0\n",
       "41  ID00042       1\n",
       "42  ID00043       0\n",
       "43  ID00044       0\n",
       "44  ID00045       0\n",
       "45  ID00046       0\n",
       "46  ID00047       0\n",
       "47  ID00048       0\n",
       "48  ID00049       1\n",
       "49  ID00050       0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.pred(Train[1:50], ID = 'ID', prob = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       0\n",
       "11       0\n",
       "12       0\n",
       "13       1\n",
       "14       0\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       1\n",
       "24       0\n",
       "25       1\n",
       "26       1\n",
       "27       0\n",
       "28       0\n",
       "29       1\n",
       "30       0\n",
       "31       1\n",
       "32       1\n",
       "33       1\n",
       "34       0\n",
       "35       1\n",
       "36       0\n",
       "37       0\n",
       "38       1\n",
       "39       0\n",
       "40       1\n",
       "41       0\n",
       "42       0\n",
       "43       0\n",
       "44       0\n",
       "45       0\n",
       "46       0\n",
       "47       1\n",
       "48       0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.pred(Train[1:50].drop(['ID'], axis = 1), ID = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "                                Total          %\n",
      "Grid_voltage                   101322  16.411451\n",
      "Grid_voltage_std               101322  16.411451\n",
      "Grid_voltage_max               101322  16.411451\n",
      "Grid_voltage_min               101322  16.411451\n",
      "Gearbox_inlet_temperature        8064   1.306152\n",
      "Generator_converter_speed        8064   1.306152\n",
      "Generator_converter_speed_min    8064   1.306152\n",
      "Generator_converter_speed_max    8064   1.306152\n",
      "Generator_converter_speed_std    8064   1.306152\n",
      "Gearbox_inlet_temperature_min    8064   1.306152\n",
      "Gearbox_inlet_temperature_max    8064   1.306152\n",
      "Gearbox_inlet_temperature_std    8064   1.306152\n",
      "Absolute_wind_direction_c          72   0.011662\n",
      "Nacelle_angle_c                    72   0.011662 \n",
      "\n",
      "NaN data filled by 0 \n",
      "\n",
      "Searching for the best regressor on 1000 data using r2 loss... \n",
      "\n",
      "\n",
      " Bagging: 0.9723 (+/- 0.0135)\n",
      "\n",
      " Gradient Boosting: 0.9756 (+/- 0.0125)\n",
      "\n",
      " XGBoost: 0.9761 (+/- 0.0140)\n",
      "\n",
      " Random Forest: 0.9700 (+/- 0.0204)\n",
      "\n",
      " Decision Tree: 0.9341 (+/- 0.0185)\n",
      "\n",
      " Extra Tree: 0.9740 (+/- 0.0090)\n",
      "\n",
      " KNN: 0.6040 (+/- 0.0909)\n",
      "\n",
      " SVM: -0.1360 (+/- 0.0263)\n",
      "\n",
      " Searching for the best hyperparametres of XGBoost on 500 data among : \n",
      "\n",
      "{'eta': [0.01, 0.1, 0.3], 'max_depth': [5, 10, 15], 'gamma': [0, 0.1, 0.01]} \n",
      "\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, the best estimator is : XGBoost regressor\n",
      "\n",
      " Using these hyperparametres : {'eta': 0.01, 'gamma': 0.01, 'max_depth': 5}\n",
      "\n",
      " With this r2 score : 0.958793882476215\n"
     ]
    }
   ],
   "source": [
    "clf = BestEstimator(type_esti = 'regressor', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3)\n",
    "\n",
    "clf.fit(Train, Target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 1000,\n",
    "            n_grid = 500,\n",
    "            value_nan = 0,\n",
    "            view_nan = True,\n",
    "           params = False,\n",
    "        scoring = 'r2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = clf.ReFit(Train[0:50], Target[0:50], ID = 'ID', target_ID = 'ID', value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = GridSearchCV(param_grid = {'n_estimators': [300],\n",
    "                                      'max_depth': [5, 10],\n",
    "                                      'criterion': ['mse', 'mae']},\n",
    "                  estimator = RandomForestRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [300], 'max_depth': [5, 10], 'criterion': ['mse', 'mae']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.fit(clf.Transform(Train[0:100], ID = 'ID', value=0), clf.Transform(Target[0:100], ID ='ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-90a5700417ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "gr.get_params().transform(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'WT1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-c3bf2c462afa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'WT1'"
     ]
    }
   ],
   "source": [
    "gr.best_estimator_.fit(Train,Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
