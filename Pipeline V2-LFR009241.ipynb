{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, \\\n",
    "    RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return (roc_auc_score(y_test, y_pred, average=average))\n",
    "\n",
    "\n",
    "class BestEstimator(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 type_esti='Classifier',\n",
    "                 cv=3,\n",
    "                 grid=True,\n",
    "                 hard_grid=False,\n",
    "                 cv_grid=3):\n",
    "\n",
    "        self.type_esti = type_esti\n",
    "        self.cv = cv\n",
    "        self.grid = grid\n",
    "        self.hard_grid = hard_grid\n",
    "        self.cv_grid = cv_grid\n",
    "\n",
    "        self.Decision_Function = None\n",
    "        self.gr = None\n",
    "        self.estim = None\n",
    "        self.Target = None\n",
    "        self.Data = None\n",
    "        self.le = None\n",
    "        self.lab = None\n",
    "        self.best_score = None\n",
    "\n",
    "    def fit(self, data, target,\n",
    "            ID='ID',\n",
    "            target_ID=True,\n",
    "            n=1000,\n",
    "            n_grid=1000,\n",
    "            view_nan=True,\n",
    "            value=0,\n",
    "            scoring='roc_auc'):\n",
    "\n",
    "        loss = scoring\n",
    "        self.Data = data.copy()\n",
    "        self.Target = target.copy()\n",
    "\n",
    "        self.Data.drop([ID], axis=1, inplace=True)\n",
    "        if target_ID:\n",
    "            self.Target.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "        if view_nan:\n",
    "            print(\"Missing Values :\\n\")\n",
    "\n",
    "            total = self.Data.isnull().sum().sort_values(ascending=False)\n",
    "            percent = (self.Data.isnull().sum() / self.Data.isnull().count()).sort_values(ascending=False) * 100\n",
    "            missing_data = pd.concat([total, percent], axis=1, keys=['Total', '%'])\n",
    "            print(\"{} \\n\".format(missing_data[(percent > 0)]))\n",
    "\n",
    "        if type(value) == int:\n",
    "            self.Data.fillna(value, inplace=True)\n",
    "            # self.Test.fillna(value, inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        elif value == 'bfill':\n",
    "            self.Data.fillna('bfill', inplace=True)\n",
    "            # self.Test.fillna('bfill', inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        elif value == 'ffill':\n",
    "            self.Data.fillna('ffill', inplace=True)\n",
    "            # self.Test.fillna('ffill', inplace = True)\n",
    "            # self.Missing_values()\n",
    "\n",
    "        if self.Data.isnull().any().any() == False:\n",
    "            print('NaN data filled by {} \\n'.format(value))\n",
    "        else:\n",
    "            print('Fail to fill NaN data')\n",
    "\n",
    "        for i in self.Data.columns:  ###########\n",
    "\n",
    "            if self.Data[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(self.Data[i]))\n",
    "                self.Data[i] = encoder.transform(list(self.Data[i]))\n",
    "\n",
    "            if self.Data[i].dtype == float:\n",
    "                self.Data[i] = self.Data[i].astype('int')\n",
    "\n",
    "\n",
    "        for i in self.Target.columns:\n",
    "            if self.Target[i].dtype == object:\n",
    "                #self.cat = True\n",
    "                self.le = LabelEncoder()\n",
    "                self.le.fit(list(self.Target[i]))\n",
    "                self.Target[i] = self.le.transform(list(self.Target[i]))\n",
    "\n",
    "        X_tr, X_te, Y_tr, Y_te = train_test_split(self.Data, self.Target, random_state=0, test_size=1 / 3)\n",
    "\n",
    "        print('Searching for the best regressor on {} data using {} loss... \\n'.format(n, scoring))\n",
    "\n",
    "        if self.type_esti == 'Classifier':\n",
    "\n",
    "            # print('\\n Searching for the best classifier on {} data... \\n'.format(n))\n",
    "\n",
    "            clfs = {}\n",
    "            clfs['Bagging'] = {'clf': BaggingClassifier(), 'name': 'Bagging'}\n",
    "            clfs['Gradient Boosting'] = {'clf': GradientBoostingClassifier(), 'name': 'Gradient Boosting'}\n",
    "            clfs['XGBoost'] = {'clf': XGBClassifier(), 'name': 'XGBoost'}\n",
    "            clfs['Random Forest'] = {'clf': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                                     'name': 'Random Forest'}\n",
    "            clfs['Decision Tree'] = {'clf': DecisionTreeClassifier(), 'name': 'Decision Tree'}\n",
    "            clfs['Extra Tree'] = {'clf': ExtraTreesClassifier(n_jobs=-1), 'name': 'Extra Tree'}\n",
    "\n",
    "            clfs['KNN'] = {'clf': KNeighborsClassifier(n_jobs=-1), 'name': 'KNN'}\n",
    "            # clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "            # clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "            clfs['SVM'] = {'clf': SVC(gamma='auto'), 'name': 'SVM'}\n",
    "\n",
    "            # if scoring == 'AUC' and np.unique(self.Target).shape[0] > 2:\n",
    "            #    scoring = self.AUC\n",
    "            #   score = 'AUC'\n",
    "            # elif scoring == 'AUC':                #########################################\n",
    "            #   score = 'AUC'     ##########################################\n",
    "            #  scoring = 'roc_auc' #########################################\n",
    "\n",
    "            for item in clfs:\n",
    "                Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.ravel(Y_tr[0:n]),\n",
    "                                        cv=self.cv, scoring=scoring)\n",
    "\n",
    "                Score_mean = Score.mean()\n",
    "                STD2 = Score.std() * 2\n",
    "\n",
    "                clfs[item]['score'] = Score  # roc_auc\n",
    "                clfs[item]['mean'] = Score_mean\n",
    "                clfs[item]['std2'] = STD2\n",
    "\n",
    "                print(\"\\n {}\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(),\n",
    "                                                                     clfs[item]['score'].std() * 2)))\n",
    "\n",
    "            Best_clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['name']\n",
    "\n",
    "\n",
    "\n",
    "        elif self.type_esti == 'Regressor':\n",
    "\n",
    "            clfs = {}\n",
    "            clfs['Bagging'] = {'clf': BaggingRegressor(), 'name': 'Bagging'}\n",
    "            clfs['Gradient Boosting'] = {'clf': GradientBoostingRegressor(), 'name': 'Gradient Boosting'}\n",
    "            clfs['XGBoost'] = {'clf': XGBRegressor(), 'name': 'XGBoost'}\n",
    "            clfs['Random Forest'] = {'clf': RandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "                                     'name': 'Random Forest'}\n",
    "            clfs['Decision Tree'] = {'clf': DecisionTreeRegressor(), 'name': 'Decision Tree'}\n",
    "            clfs['Extra Tree'] = {'clf': ExtraTreesRegressor(n_jobs=-1), 'name': 'Extra Tree'}\n",
    "            clfs['KNN'] = {'clf': KNeighborsRegressor(n_jobs=-1), 'name': 'KNN'}\n",
    "            # clfs['NN'] = {'clf': MLPClassifier(), 'name': 'MLPClassifier'\n",
    "            # clfs['LR'] = {'clf': LogisticClassifier(), 'name': 'LR'}\n",
    "            clfs['SVM'] = {'clf': SVR(gamma='auto'), 'name': 'SVM'}\n",
    "\n",
    "            for item in clfs:\n",
    "                # print(Y_tr[0:30])\n",
    "                Score = cross_val_score(clfs[item]['clf'], np.asarray(X_tr[0:n]), np.array(np.ravel(Y_tr[0:n])),\n",
    "                                        cv=self.cv, scoring=scoring)\n",
    "                Score_mean = Score.mean()\n",
    "                STD2 = Score.std() * 2\n",
    "\n",
    "                clfs[item]['score'] = Score  # roc_auc\n",
    "                clfs[item]['mean'] = Score_mean\n",
    "                clfs[item]['std2'] = STD2\n",
    "\n",
    "                print(\"\\n {}\".format(item + \": %0.4f (+/- %0.4f)\" % (clfs[item]['score'].mean(),\n",
    "                                                                     clfs[item]['score'].std() * 2)))\n",
    "\n",
    "            Best_clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['name']\n",
    "\n",
    "        if self.grid:\n",
    "            # print('grid = True')\n",
    "\n",
    "\n",
    "\n",
    "            if self.hard_grid == False:\n",
    "\n",
    "                if Best_clf == 'Extra Tree':\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600],\n",
    "                                  'criterion': ['mse', 'mae'],\n",
    "                                  'max_depth': [None, 5, 10]}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600],\n",
    "                                  'criterion': ['gini', 'entropy'],\n",
    "                                  'max_depth': [None, 5, 10]}\n",
    "\n",
    "                if Best_clf == 'Gradient Boosting':\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600],\n",
    "                                  'max_depth': [5, 10, None],\n",
    "                                  'learning_rate': [.001, .01, .1],\n",
    "                                  'loss': ['ls', 'lad']}\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600],\n",
    "                                  'max_depth': [5, 10, None],\n",
    "                                  'learning_rate': [.001, .01, .1],\n",
    "                                  'loss': ['deviance', 'exponential']}\n",
    "\n",
    "\n",
    "                elif Best_clf == 'Random Forest':\n",
    "                    #  print('Best_clf = dt ou rf')\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300],\n",
    "                                  'max_depth': [5, 10, None],\n",
    "                                  'criterion': ['mse', 'mae']}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300],\n",
    "                                  'max_depth': [5, 10, None],\n",
    "                                  'criterion': ['gini', 'entropy']}\n",
    "\n",
    "                elif Best_clf == 'Decision Tree':\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'max_depth': [5, 10, 50, None],\n",
    "                                  'criterion': ['mse', 'friedman_mse', 'mae']}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'max_depth': [5, 10, 50, None],\n",
    "                                  'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "                elif Best_clf == 'XGBoost':\n",
    "                    # print('Best_clf = xgb')\n",
    "\n",
    "                    params = {'eta': [.01, .1, .3],\n",
    "                              'max_depth': [5, 10, 15],\n",
    "                              'gamma': [0, .1, .01]}\n",
    "\n",
    "                elif Best_clf == 'Bagging':\n",
    "                    # print('best_clf = bag)')\n",
    "\n",
    "                    params = {'n_estimators': [100, 300, 600]}\n",
    "\n",
    "                elif Best_clf == 'KNN':\n",
    "\n",
    "                    params = {'n_neighbors': [2, 5, 10, 30, 40],\n",
    "                              'p': [1, 2]}\n",
    "\n",
    "                elif Best_clf == 'SVM':\n",
    "\n",
    "                    params = {'C': {1, .5, .1, 5},\n",
    "                              'tol': [.01, .001, .1, .0001]}\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                if Best_clf == 'Extra Tree':\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                  'criterion': ['mae', 'mse'],\n",
    "                                  'max_depth': [None, 5, 10, 15, 20, 25]}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                  'criterion': ['gini', 'entropy'],\n",
    "                                  'max_depth': [None, 5, 10, 15, 20, 25]}\n",
    "\n",
    "                if Best_clf == 'Gradient Boosting':\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600, 1000, 1200],\n",
    "                                  'max_depth': [5, 10, 15, 25, None],\n",
    "                                  'learning_rate': [.001, .01, .1],\n",
    "                                  'loss': ['ls', 'lad', 'huber', 'quantile'],\n",
    "                                  'criterion': ['mse', 'friedman_mse']}\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [100, 300, 600, 1000, 1200],\n",
    "                                  'max_depth': [5, 10, 15, 25, None],\n",
    "                                  'learning_rate': [.001, .01, .1],\n",
    "                                  'loss': ['deviance', 'exponential'],\n",
    "                                  'criterion': ['mse', 'friedman_mse']}\n",
    "\n",
    "\n",
    "                elif Best_clf == 'Random Forest':\n",
    "                    #  print('Best_clf = dt ou rf')\n",
    "\n",
    "                    if self.type_esti == 'Regressor':\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                  'max_depth': [5, 10, 15, 20, 25, None],\n",
    "                                  'criterion': ['mse', 'mae']}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'n_estimators': [10, 100, 300, 600, 1000, 1200],\n",
    "                                  'max_depth': [5, 10, 15, 20, 25],\n",
    "                                  'criterion': ['gini', 'entropy']}\n",
    "\n",
    "                elif Best_clf == 'Decision Tree':\n",
    "\n",
    "                    if params == 'Regressor':\n",
    "\n",
    "                        params = {'max_depth': [5, 10, 50, 100, None],\n",
    "                                  'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "                                  'splitter': ['best', 'random']}\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        params = {'max_depth': [5, 10, 50, 100, None],\n",
    "                                  'criterion': ['gini', 'entropy'],\n",
    "                                  'splitter': ['best', 'random']}\n",
    "\n",
    "\n",
    "                elif Best_clf == 'XGBoost':\n",
    "                    # print('Best_clf = xgb')\n",
    "\n",
    "                    params = {'eta': [0.001, .01, .1, .3, 1],\n",
    "                              'max_depth': [5, 10, 15, 20, 25],\n",
    "                              'gamma': [0, .1, .01, .001]}\n",
    "\n",
    "                elif Best_clf == 'Bagging':\n",
    "                    # print('best_clf = bag)')\n",
    "\n",
    "                    params = {'n_estimators': [100, 300, 600, 1000, 1200, 1500]}\n",
    "\n",
    "                elif Best_clf == 'KNN':\n",
    "\n",
    "                    params = {'n_neighbors': [2, 5, 10, 30, 40, 70, 100],\n",
    "                              'p': [1, 2, 3]}\n",
    "\n",
    "                elif Best_clf == 'SVM':\n",
    "\n",
    "                    params = {'C': {1, .5, .1, 5, .01, .001},\n",
    "                              'tol': [.01, .001, .1, .0001, 1],\n",
    "                              'kernel': ['rbf', 'linear', 'poly', 'sigmoid', 'precomputed']}\n",
    "\n",
    "            if self.hard_grid:\n",
    "                print('\\n Searching for the best hyperparametres of {} using hard_grid on {} data among : \\n'.format(\n",
    "                    Best_clf, n_grid))\n",
    "\n",
    "            else:\n",
    "                print('\\n Searching for the best hyperparametres of {} on {} data among : \\n'.format(Best_clf, n_grid))\n",
    "            print('{} \\n'.format(params))\n",
    "            # print('Starting GridSearchCV using {} Classifier with {} folds \\n'.format(Best_clf, cv_grid))\n",
    "\n",
    "            clf = clfs[max(clfs.keys(), key=(lambda k: clfs[k]['mean']))]['clf']\n",
    "\n",
    "            self.gr = GridSearchCV(clf, param_grid=params, cv=self.cv_grid, scoring=scoring,\n",
    "                                   verbose=1, refit=True, iid=True, n_jobs=-1)\n",
    "\n",
    "            self.gr.fit(X_tr[0:n_grid], np.ravel(Y_tr[0:n_grid]))\n",
    "\n",
    "            print('\\n In the end, the best estimator is : {} {}'.format(Best_clf, self.type_esti))\n",
    "\n",
    "            print('\\n Using these hyperparametres : {}'.format(self.gr.best_params_))\n",
    "\n",
    "            print('\\n With this {} score : {}'.format(loss, self.gr.best_score_))\n",
    "\n",
    "            self.Decision_Function = self.gr.best_estimator_\n",
    "\n",
    "            self.best_score = self.gr.best_score_\n",
    "\n",
    "            #print(self.best_score)\n",
    "\n",
    "            self.lab = self.le.inverse_transform(self.gr.classes_)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('\\n Best {} : {}'.format(self.type_esti, Best_clf))\n",
    "\n",
    "\n",
    "\n",
    "    def ReFit(self, Train, Target, ID='ID', target_ID=True, value=0):\n",
    "\n",
    "        train = self.Transform(Train, value = value, ID = ID)\n",
    "        #target = self.Transform(Target, value = value, ID = ID)\n",
    "        target = Target.copy()\n",
    "\n",
    "        if target_ID == True:\n",
    "            target.drop([ID], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "        self.estim = self.Decision_Function.fit(train, np.ravel(target))\n",
    "\n",
    "        return (self.estim)\n",
    "\n",
    "\n",
    "\n",
    "    def Transform(self, Data, value=0, ID='ID'):\n",
    "\n",
    "        Test = Data.copy()\n",
    "\n",
    "        if ID != None:\n",
    "            Test.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "        if type(value) == int:\n",
    "            Test.fillna(value, inplace=True)\n",
    "\n",
    "        elif value == 'bfill':\n",
    "            Test.fillna('bfill', inplace=True)\n",
    "\n",
    "        elif value == 'ffill':\n",
    "            Test.fillna('ffill', inplace=True)\n",
    "\n",
    "        for i in Test.columns:  ###########\n",
    "            if Test[i].dtype == float:\n",
    "                Test[i] = Test[i].astype('int')\n",
    "\n",
    "            elif Test[i].dtype == object:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(list(Test[i]))\n",
    "                Test[i] = encoder.transform(list(Test[i]))\n",
    "        return (Test)\n",
    "\n",
    "\n",
    "\n",
    "    def pred_grid(self, Test, ID='ID', value=0, prob=False):\n",
    "\n",
    "        if ID == None:\n",
    "            test = self.Transform(Test, ID=None, value=value)\n",
    "        else:\n",
    "            test = self.Transform(Test, ID=ID, value=value)\n",
    "\n",
    "        if prob == False:\n",
    "            pred = pd.DataFrame()\n",
    "            predict = self.le.inverse_transform(self.gr.predict(test))\n",
    "\n",
    "            if ID == None:\n",
    "                pred['Target'] = predict\n",
    "            else:\n",
    "                pred[ID] = Test[ID]\n",
    "                pred['Target'] = predict\n",
    "\n",
    "        else:\n",
    "            if ID == None:\n",
    "                #pred = pd.DataFrame(self.gr.predict_proba(test), columns=self.le.inverse_transform(self.gr.classes_))\n",
    "                pred = pd.DataFrame(self.gr.predict_proba(test), columns=self.lab)\n",
    "\n",
    "            else:\n",
    "                #pred = pd.DataFrame(self.gr.predict_proba(test), columns=self.le.inverse_transform(self.gr.classes_))\n",
    "                pred = pd.DataFrame(self.gr.predict_proba(test), columns=self.lab)\n",
    "                pred.insert(loc=0, column=ID, value=Test[ID])\n",
    "\n",
    "        return (pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pred(self, Test, ID=None, value=0, target_ID=None, n=1000, prob=False):\n",
    "\n",
    "        #if ID == None:\n",
    "        test = self.Transform(Test, ID=ID, value=value)\n",
    "\n",
    "        #else:\n",
    "         #   test = self.Transform(Test, ID=ID, value=0)\n",
    "\n",
    "        if self.estim == None:\n",
    "            self.estim = self.ReFit(self.Data[0:n], self.Target[0:n], ID=None, value=value, target_ID = False)\n",
    "\n",
    "            if prob == False:\n",
    "                pred = pd.DataFrame()\n",
    "                predict = self.le.inverse_transform(self.estim.predict(test))  ###########\n",
    "\n",
    "                if ID == None:\n",
    "                    pred['Target'] = predict\n",
    "                else:\n",
    "                    pred[ID] = Test[ID]\n",
    "                    pred['Target'] = predict\n",
    "\n",
    "            else:\n",
    "                if ID == None:\n",
    "                    pred = pd.DataFrame(self.estim.predict_proba(test), columns=self.lab)\n",
    "\n",
    "                else:\n",
    "                    pred = pd.DataFrame(self.estim.predict_proba(test), columns=self.lab)\n",
    "                    pred.insert(loc=0, column=ID, value=Test[ID])\n",
    "\n",
    "\n",
    "        else:\n",
    "            pred = pd.DataFrame()\n",
    "            if prob == False:\n",
    "\n",
    "                if prob == False:\n",
    "                    pred = pd.DataFrame()\n",
    "                    predict = self.estim.predict(test)\n",
    "\n",
    "                    if ID == None:\n",
    "                        pred['Target'] = predict\n",
    "                    else:\n",
    "                        pred[ID] = Test[ID]\n",
    "                        pred['Target'] = predict\n",
    "\n",
    "            else:\n",
    "                if ID == None:\n",
    "                    pred = pd.DataFrame(self.estim.predict_proba(test), columns=self.lab)\n",
    "\n",
    "                else:\n",
    "                    pred = pd.DataFrame(self.estim.predict_proba(test), columns=self.lab)\n",
    "                    pred.insert(loc=0, column=ID, value=Test[ID])\n",
    "\n",
    "        return (pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def custom_grid(self, Train, Target, ID='ID', target_ID=True,\n",
    "                    n=1000, metric='AUC', params=None, cv=3, DF=None, value=0):\n",
    "\n",
    "        target = Target.copy()\n",
    "        loss = metric\n",
    "\n",
    "        for i in target.columns:\n",
    "            if target[i].dtype == object:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(list(target[i]))\n",
    "                target[i] = le.transform(list(target[i]))\n",
    "\n",
    "        if ID != None:\n",
    "            train = self.Transform(Train, ID=ID, value=value)\n",
    "            if target_ID:\n",
    "                target.drop([ID], axis=1, inplace=True)\n",
    "        if DF == None:\n",
    "            DF = self.Decision_Function\n",
    "\n",
    "        # if metric == 'AUC':\n",
    "        #   metric = self.AUC\n",
    "        #  loss = 'AUC'\n",
    "\n",
    "        gr = GridSearchCV(DF, param_grid=params, cv=cv, scoring=metric, n_jobs=-1,\n",
    "                          verbose=1, refit=True, iid=True);\n",
    "\n",
    "        gr.fit(train[0:n], np.ravel(target[0:n]))\n",
    "\n",
    "        print('\\n Best hyperparametres : {}'.format(gr.best_params_))\n",
    "\n",
    "        print('\\n Giving this {} score : {}'.format(loss, gr.best_score_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Bagg_fit(self, Train, Target, n_estimators = None, type_esti = 'Classifier', n = 1000,\n",
    "                 cv = 3, value = 0, ID = None, metric = None):\n",
    "\n",
    "        params = {'n_estimators' : n_estimators}\n",
    "\n",
    "        train = self.Transform(Train, value = value, ID = ID)\n",
    "        target = self.Transform(Target, value = value, ID = ID)\n",
    "\n",
    "        Best_DF = self.Decision_Function\n",
    "\n",
    "        if type_esti == 'Classifier':\n",
    "            esti = BaggingClassifier(base_estimator = Best_DF)\n",
    "        elif type_esti == 'Regressor':\n",
    "            esti = BaggingRegressor(base_estimator = Best_DF)\n",
    "\n",
    "\n",
    "        DF =  GridSearchCV(estimator = esti, param_grid = params, n_jobs = -1, verbose = 1)\n",
    "\n",
    "        DF.fit(train[0:n], np.ravel(target[0:n]))\n",
    "\n",
    "        print('\\n Best hyperparametres : {}'.format(DF.best_params_))\n",
    "\n",
    "        print('\\n Giving this {} score : {}'.format(metric, DF.best_score_))\n",
    "\n",
    "        if self.gr.best_score_ < DF.best_score_:\n",
    "            self.Decision_Function = DF.best_estimator_\n",
    "\n",
    "        #print(self.Decision_Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i bestestimator_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Train.csv\", sep = ',')\n",
    "Target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "                      Total       %\n",
      "WARRANTIES_PRICE      96603  96.603\n",
      "SHIPPING_PRICE        67610  67.610\n",
      "BUYER_BIRTHDAY_DATE    5836   5.836\n",
      "SHIPPING_MODE           315   0.315\n",
      "PRICECLUB_STATUS         57   0.057\n",
      "SELLER_SCORE_AVERAGE      6   0.006\n",
      "SELLER_SCORE_COUNT        6   0.006 \n",
      "\n",
      "NaN data filled by 0 \n",
      "\n",
      "Searching for the best regressor on 1000 data using accuracy loss... \n",
      "\n",
      "\n",
      " Bagging: 0.4690 (+/- 0.0080)\n",
      "\n",
      " Gradient Boosting: 0.4680 (+/- 0.0119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " XGBoost: 0.4900 (+/- 0.0156)\n",
      "\n",
      " Random Forest: 0.5061 (+/- 0.0273)\n",
      "\n",
      " Decision Tree: 0.3730 (+/- 0.0072)\n",
      "\n",
      " Extra Tree: 0.4650 (+/- 0.0155)\n",
      "\n",
      " KNN: 0.4470 (+/- 0.0126)\n",
      "\n",
      " SVM: 0.4870 (+/- 0.0027)\n",
      "\n",
      " Searching for the best hyperparametres of Random Forest on 1000 data among : \n",
      "\n",
      "{'n_estimators': [10, 100, 300], 'max_depth': [5, 10, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   19.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " In the end, the best estimator is : Random Forest Classifier\n",
      "\n",
      " Using these hyperparametres : {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 300}\n",
      "\n",
      " With this accuracy score : 0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "clf = BestEstimator(type_esti = 'Classifier', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3)\n",
    "\n",
    "clf.fit(Train, Target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 1000,\n",
    "            n_grid = 1000,\n",
    "            view_nan = True,\n",
    "        scoring = 'accuracy',\n",
    "       value = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   30.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   30.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparametres : {'n_estimators': 10}\n",
      "\n",
      " Giving this accuracy score : 0.5133333333333333\n",
      "BaggingClassifier(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf.Bagg_fit(Train, Target, n_estimators = [10,40], ID = 'ID', metric = 'accuracy', n = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bestestimator_v2.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Train1.csv\", sep = ';')\n",
    "target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target1.csv\", sep = ';')\n",
    "\n",
    "Target = pd.DataFrame()\n",
    "Target['ID'] = target['ID']\n",
    "Target['Target'] = [np.nan]*target.shape[0]\n",
    "\n",
    "\n",
    "for i in range(target.shape[0]):\n",
    "    if target['Target'][i] == 1:\n",
    "        Target['Target'][i] ='+'\n",
    "    else :\n",
    "        Target['Target'][i] ='-'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN data filled by 0 \n",
      "\n",
      "Searching for the best regressor on 1000 data using roc_auc loss... \n",
      "\n",
      "\n",
      " Bagging: 0.7116 (+/- 0.0077)\n",
      "\n",
      " Gradient Boosting: 0.7215 (+/- 0.0355)\n",
      "\n",
      " XGBoost: 0.7121 (+/- 0.0225)\n",
      "\n",
      " Random Forest: 0.7269 (+/- 0.0401)\n",
      "\n",
      " Decision Tree: 0.6278 (+/- 0.0293)\n",
      "\n",
      " Extra Tree: 0.6797 (+/- 0.0246)\n",
      "\n",
      " KNN: 0.6367 (+/- 0.0594)\n",
      "\n",
      " SVM: 0.5040 (+/- 0.0054)\n",
      "\n",
      " Searching for the best hyperparametres of Random Forest on 1000 data among : \n",
      "\n",
      "{'n_estimators': [10, 100, 300], 'max_depth': [5, 10, None], 'criterion': ['gini', 'entropy']} \n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, the best estimator is : Random Forest classifier\n",
      "\n",
      " Using these hyperparametres : {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 300}\n",
      "\n",
      " With this roc_auc score : 0.7407524989135159\n"
     ]
    }
   ],
   "source": [
    "clf = BestEstimator(type_esti = 'classifier', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3)\n",
    "\n",
    "clf.fit(Train, Target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 1000,\n",
    "            n_grid = 1000,\n",
    "            view_nan = False,\n",
    "        scoring = 'roc_auc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparametres : {'n_estimators': 100}\n",
      "\n",
      " Giving this roc_auc score : 0.718\n"
     ]
    }
   ],
   "source": [
    "clf.Bagg_fit(Train, Target, n_estimators = [10,20,100], ID = 'ID', metric = 'roc_auc', n = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\input_training.csv\", sep = ';')\n",
    "Target = pd.read_csv(r\"C:\\Users\\jecombe\\OneDrive - Capgemini\\Notebooks\\Target_Engie.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values :\n",
      "\n",
      "                                Total          %\n",
      "Grid_voltage                   101322  16.411451\n",
      "Grid_voltage_std               101322  16.411451\n",
      "Grid_voltage_max               101322  16.411451\n",
      "Grid_voltage_min               101322  16.411451\n",
      "Gearbox_inlet_temperature        8064   1.306152\n",
      "Generator_converter_speed        8064   1.306152\n",
      "Generator_converter_speed_min    8064   1.306152\n",
      "Generator_converter_speed_max    8064   1.306152\n",
      "Generator_converter_speed_std    8064   1.306152\n",
      "Gearbox_inlet_temperature_min    8064   1.306152\n",
      "Gearbox_inlet_temperature_max    8064   1.306152\n",
      "Gearbox_inlet_temperature_std    8064   1.306152\n",
      "Absolute_wind_direction_c          72   0.011662\n",
      "Nacelle_angle_c                    72   0.011662 \n",
      "\n",
      "NaN data filled by 0 \n",
      "\n",
      "Searching for the best regressor on 500 data using r2 loss... \n",
      "\n",
      "\n",
      " Bagging: 0.9634 (+/- 0.0235)\n",
      "\n",
      " Gradient Boosting: 0.9721 (+/- 0.0141)\n",
      "\n",
      " XGBoost: 0.9666 (+/- 0.0235)\n",
      "\n",
      " Random Forest: 0.9708 (+/- 0.0170)\n",
      "\n",
      " Decision Tree: 0.9574 (+/- 0.0336)\n",
      "\n",
      " Extra Tree: 0.9653 (+/- 0.0192)\n",
      "\n",
      " KNN: 0.4415 (+/- 0.0731)\n",
      "\n",
      " SVM: -0.1587 (+/- 0.1594)\n",
      "\n",
      " Searching for the best hyperparametres of Gradient Boosting on 500 data among : \n",
      "\n",
      "{'n_estimators': [100, 300, 600], 'max_depth': [5, 10, None], 'learning_rate': [0.001, 0.01, 0.1], 'loss': ['ls', 'lad']} \n",
      "\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finally, the best estimator is : Gradient Boosting regressor\n",
      "\n",
      " Using these hyperparametres : {'learning_rate': 0.01, 'loss': 'ls', 'max_depth': 5, 'n_estimators': 600}\n",
      "\n",
      " With this r2 score : 0.9750078443220153\n"
     ]
    }
   ],
   "source": [
    "clf = BestEstimator(type_esti = 'regressor', \n",
    "                 cv = 3, \n",
    "                 grid = True, \n",
    "                 hard_grid = False,\n",
    "                 cv_grid = 3)\n",
    "\n",
    "clf.fit(Train, Target,\n",
    "            ID = 'ID',\n",
    "            target_ID = True,\n",
    "            n = 500,\n",
    "            n_grid = 500,\n",
    "            view_nan = True,\n",
    "        scoring = 'r2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   43.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   43.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best hyperparametres : {'n_estimators': 20}\n",
      "\n",
      " Giving this r2 score : 0.1628245956749507\n"
     ]
    }
   ],
   "source": [
    "clf.Bagg_fit(Train, Target, n_estimators = [10,20], ID = 'ID', metric = 'r2', n = 500, type_esti = 'Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
